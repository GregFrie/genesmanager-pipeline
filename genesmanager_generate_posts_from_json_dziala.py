import os
import json
import time
import requests
import traceback
from bs4 import BeautifulSoup
from openai import OpenAI

# Wymaga ustawionej zmiennej Å›rodowiskowej OPENAI_API_KEY
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”¹ Normalizacja tytuÅ‚u po polsku
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def format_polish_title(raw_title: str) -> str:
    raw_title = raw_title.strip()
    if raw_title.lower().startswith("tytuÅ‚:"):
        raw_title = raw_title[6:].strip()
    if not raw_title:
        return ""
    words = raw_title.split()
    if not words:
        return ""
    words[0] = words[0].capitalize()
    return " ".join(words)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”¹ Pobieranie treÅ›ci artykuÅ‚u ze strony ÅºrÃ³dÅ‚owej
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_content_from_url(url):
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        if "gov.pl" in url:
            main = soup.find("div", class_="editor-content")
        elif "serwiszoz.pl" in url:
            main = soup.find("div", class_="blog-content")
        elif "rynekzdrowia.pl" in url:
            main = soup.find("article")
        elif "nfz.gov.pl" in url:
            main = soup.find("div", class_="main-content") or soup.find("article")
        else:
            main = soup.find("div", class_="content") \
                   or soup.find("div", class_="article-content") \
                   or soup.find("div", class_="entry-content")

        if not main:
            return ""

        paragraphs = main.find_all("p")
        return "\n".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))

    except Exception as e:
        print(f"âŒ BÅ‚Ä…d pobierania treÅ›ci z URL: {url} â†’ {e}")
        traceback.print_exc()
        return ""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”¹ Czyszczenie nazw plikÃ³w
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def sanitize_filename(name):
    return "".join(c if c.isalnum() or c in " _-" else "_" for c in name).strip()[:60]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”¹ Generowanie postÃ³w na podstawie listy artykuÅ‚Ã³w
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def generate_posts(articles):
    output_dir = "output_posts"
    os.makedirs(output_dir, exist_ok=True)
    processed_urls = set()

    for i, article in enumerate(articles):
        try:
            url = article.get("url", "").strip()
            if not url or url in processed_urls:
                continue
            processed_urls.add(url)

            title = format_polish_title(article.get("title", "").strip())
            lead = article.get("lead", "").strip()
            content = extract_content_from_url(url)
            if not content and lead:
                content = lead
            if not content:
                print(f"âš ï¸ ArtykuÅ‚ {i+1}: brak treÅ›ci â€“ fallback do komunikatu zastÄ™pczego.")
                content = f"Aktualizacja z serwisu {article.get('source','nieznane ÅºrÃ³dÅ‚o')} â€“ brak szczegÃ³Å‚owej treÅ›ci na stronie ÅºrÃ³dÅ‚owej."

            prompt = (
                "Napisz ekspercki, ale przystÄ™pny artykuÅ‚ blogowy na podstawie poniÅ¼szego tekstu ÅºrÃ³dÅ‚owego. "
                "ArtykuÅ‚ ma byÄ‡ unikalny, inspirowany treÅ›ciÄ…, ale nie moÅ¼e jej kopiowaÄ‡. "
                "Ma byÄ‡ przeznaczony dla wÅ‚aÅ›cicieli i managerÃ³w podmiotÃ³w medycznych.\n\n"
                f"TytuÅ‚ artykuÅ‚u: {title}\n\n"
                f"TreÅ›Ä‡ ÅºrÃ³dÅ‚owa:\n{content}"
            )

            response = client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "JesteÅ› doÅ›wiadczonym redaktorem medycznym."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7
            )

            final_text = response.choices[0].message.content.strip() if response.choices else "âš ï¸ Brak odpowiedzi od modelu"

            filename_base = sanitize_filename(title if title else "bez_tytulu")
            filename = f"{i+1:03d}_{filename_base}.txt"
            output_path = os.path.join(output_dir, filename)

            with open(output_path, "w", encoding="utf-8") as out_f:
                out_f.write(title + "\n\n" + final_text)

            print(f"âœ… Wygenerowano: {filename}")
            time.sleep(1.5)

        except Exception as e:
            print(f"âŒ BÅ‚Ä…d przy generowaniu artykuÅ‚u {i+1}: {e}")
            traceback.print_exc()

# JeÅ›li chcesz, Å¼eby skrypt mÃ³gÅ‚ byÄ‡ testowany samodzielnie:
if __name__ == "__main__":
    # PrzykÅ‚adowe dane testowe
    test_articles = [
        {"title": "Nowe wytyczne NFZ", "url": "https://www.gov.pl/web/zdrowie/wiadomosci", "lead": "NFZ opublikowaÅ‚ nowe wytyczne", "source": "NFZ"},
    ]
    generate_posts(test_articles)
